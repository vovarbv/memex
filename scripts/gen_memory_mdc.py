#!/usr/bin/env python
"""
Формирует .cursor/rules/memory.mdc по top-K задачам/сниппетам + предпочтениям.
"""
import os
import textwrap
import pathlib
import tiktoken
import logging
import sys
import argparse

from memory_utils import load_cfg, search, load_preferences, ROOT
from task_store import load_tasks

# ───────────────────────────────────────── Logging Setup ────
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ───────────────────────────────────────── Helper Formatters ────

def _format_task_for_mdc(task: dict) -> list[str]:
    """Formats a single task into a list of Markdown lines."""
    task_lines = []
    title = task.get('title', 'Untitled Task')
    progress = task.get('progress', 0)
    task_id = task.get('id', 'N/A')
    task_lines.append(f"- #{task_id} {title} — {progress}%")

    plan = task.get("plan", [])
    done_steps = task.get("done_steps", [])
    
    # Add completion summary if the task has a plan
    if plan:
        completion_summary = f"  * Completion: {len(done_steps)}/{len(plan)} steps done."
        task_lines.append(completion_summary)
    
    # Show pending steps as next steps
    pending_steps = [step for step in plan if step not in done_steps]
    if pending_steps:
        task_lines.append("  * Next steps:")
        for step in pending_steps[:3]:  # Show at most 3 next steps
            task_lines.append(f"    - {step}")
    
    # Optionally show completed steps (if any)
    if done_steps:
        # We can show all completed steps or limit to a certain number
        # Let's limit to 2 to avoid overwhelming the memory.mdc
        task_lines.append("  * Completed steps:")
        for step in done_steps[:2]:  # Show at most 2 completed steps
            task_lines.append(f"    - ✓ {step}")
        if len(done_steps) > 2:
            task_lines.append(f"    - ... and {len(done_steps) - 2} more")

    # Add the last note if available
    notes_str = task.get("notes", "")
    if notes_str:
        last_note_line = notes_str.strip().splitlines()[-1] if notes_str.strip() else ""
        if last_note_line:
            task_lines.append(f"  * Last note: {textwrap.shorten(last_note_line, width=120, placeholder='...')}")
    
    return task_lines

def _format_context_item_for_mdc(meta: dict) -> list[str]:
    """Formats a single context item (snippet or note) into a list of Markdown lines."""
    item_lines = []
    item_type = meta.get("type")
    item_text = meta.get("text", "")
    item_source = meta.get("source", "unknown") # For snippets, source might be filepath; for notes, it's 'manual' or similar
    item_id = meta.get("id", "N/A") # Relevant for notes if they have unique IDs in their metadata

    if item_type == "snippet":
        item_lines.append(f"Snippet from {item_source}:")
        item_lines.append(item_text) # item_text is the ```lang\ncode\n``` block
    elif item_type == "note":
        item_lines.append(f"Note (ID: {item_id} from {item_source}):")
        # Note text should be appended line by line if it's multiline, or as a block.
        # Assuming item_text for note is a single string that might contain newlines.
        item_lines.append(item_text)
    
    # Add a newline after each formatted item for separation, handled by joining with \n\n later if needed
    return item_lines

# ───────────────────────────────────────── Main Logic ────
def make():
    parser = argparse.ArgumentParser(description="Generate memory.mdc file for Cursor.")
    parser.add_argument("--focus", type=str, help="A specific query to focus the context item search on.")
    args = parser.parse_args()

    try:
        cfg = load_cfg()
    except Exception as e:
        logging.critical(f"Failed to load configuration: {e}. Aborting mdc generation.")
        sys.exit(1)

    max_total_tokens = cfg.get("prompt", {}).get("max_tokens", 10000)
    top_tasks_k = cfg.get("prompt", {}).get("top_k_tasks", 5)
    top_k_context = cfg.get("prompt", {}).get("top_k_context_items", 5)

    try:
        enc = tiktoken.get_encoding("cl100k_base")
    except Exception as e:
        logging.critical(f"Failed to load tiktoken encoder: {e}. Aborting.")
        sys.exit(1)

    final_mdc_parts = []
    current_total_tokens = 0

    # 0. Header
    header_block = "<!-- AUTO-GENERATED BY scripts/gen_memory_mdc.py. DO NOT EDIT MANUALLY. -->\n\n"
    header_tokens = len(enc.encode(header_block))
    if current_total_tokens + header_tokens <= max_total_tokens:
        final_mdc_parts.append(header_block)
        current_total_tokens += header_tokens
    else:
        logging.warning("Not enough token budget for even the header. MDC will be empty or malformed.")
        # Early exit or proceed to write minimal file? For now, proceed.

    # 1. Preferences
    if current_total_tokens < max_total_tokens:
        try:
            prefs = load_preferences(cfg)
            if prefs:
                prefs_lines = ["### Preferences"]
                for k, v in prefs.items():
                    prefs_lines.append(f"- **{k}**: {v}")
                prefs_lines.append("") # Blank line after section
                
                prefs_block = "\n".join(prefs_lines) + "\n" # Add a newline to the very end of the block
                prefs_tokens = len(enc.encode(prefs_block))

                if current_total_tokens + prefs_tokens <= max_total_tokens:
                    final_mdc_parts.append(prefs_block)
                    current_total_tokens += prefs_tokens
                else:
                    logging.info(f"Preferences section (approx {prefs_tokens} tokens) exceeds remaining token budget. Skipping.")
            else:
                logging.info("No preferences found or 'preferences.file' not configured.")
        except Exception as e:
            logging.error(f"Error loading preferences: {e}")

    # 2. Active Tasks
    active_yaml_tasks_data = []
    if current_total_tokens < max_total_tokens:
        try:
            all_yaml_tasks = load_tasks()
            active_yaml_tasks_data = [t for t in all_yaml_tasks if t.get("status") == "in_progress"]
            active_yaml_tasks_data = sorted(
                active_yaml_tasks_data,
                key=lambda t: (-t.get("progress", 0), t.get("updated_at", "")),
                reverse=True
            )[:top_tasks_k]

            if active_yaml_tasks_data:
                tasks_section_lines = ["### Active Tasks"]
                tasks_section_header_str = "\n".join(tasks_section_lines) + "\n" # Header plus one newline
                tasks_section_header_tokens = len(enc.encode(tasks_section_header_str))
                
                temp_task_md_parts = []
                current_tasks_section_tokens = 0

                # Check if even the section header can fit
                if current_total_tokens + tasks_section_header_tokens <= max_total_tokens:
                    for task_data in active_yaml_tasks_data:
                        task_md_lines = _format_task_for_mdc(task_data)
                        # Each task is a list of lines; join them, add newline, then find tokens
                        task_md_segment = "\n".join(task_md_lines) + "\n" 
                        task_tokens = len(enc.encode(task_md_segment))

                        # Total for section includes its own items + header, check against overall budget
                        if current_total_tokens + tasks_section_header_tokens + current_tasks_section_tokens + task_tokens <= max_total_tokens:
                            temp_task_md_parts.append(task_md_segment)
                            current_tasks_section_tokens += task_tokens
                        else:
                            logging.info(f"Task '{task_data.get('title','N/A')}' exceeds token budget. Stopping task additions.")
                            break
                    
                    if temp_task_md_parts:
                        full_tasks_section_str = tasks_section_header_str + "".join(temp_task_md_parts) + "\n" # Ensure newline after section
                        # Recalculate actual tokens for the constructed section to be precise
                        actual_full_tasks_section_tokens = len(enc.encode(full_tasks_section_str))
                        
                        # Final check with actual tokens
                        if current_total_tokens + actual_full_tasks_section_tokens <= max_total_tokens:
                            final_mdc_parts.append(full_tasks_section_str)
                            current_total_tokens += actual_full_tasks_section_tokens
                        else:
                             # This case should ideally not be hit if inner logic is correct, but as a fallback:
                            logging.warning("Constructed tasks section exceeded budget on final check. Skipping task section.")
                else:
                    logging.info("Not enough token budget for Active Tasks section header. Skipping.")
            else:
                logging.info("No active 'in_progress' tasks found in TASKS.yaml.")
        except Exception as e:
            logging.error(f"Error processing tasks for mdc: {e}")

    # Determine context_item_query (using active_yaml_tasks_data from above)
    context_item_query = ""
    if args.focus:
        context_item_query = args.focus
        logging.info(f"Using provided focus query: '{context_item_query}'")
    elif active_yaml_tasks_data:
        task_keywords = [task.get('title', '') for task in active_yaml_tasks_data]
        unique_titles = list(dict.fromkeys(filter(None, task_keywords)))
        if unique_titles:
            context_item_query = " ".join(unique_titles)
            logging.info(f"Generated context query from active tasks: '{context_item_query}'")
        else:
            context_item_query = cfg.get("prompt", {}).get("snippet_search_query", "general project context")
            logging.info(f"No active task titles to form query, using default: '{context_item_query}'")
    else:
        context_item_query = cfg.get("prompt", {}).get("snippet_search_query", "general project context")
        logging.info(f"No focus query and no active tasks, using default context query: '{context_item_query}'")

    # 3. Relevant Project Context (Snippets & Notes)
    if current_total_tokens < max_total_tokens:
        try:
            context_items_data = search(
                context_item_query,
                top_k_context,
                pred=lambda m: m.get("type") in ["snippet", "note"] and "text" in m
            )

            if context_items_data:
                context_section_lines = ["### Relevant Project Context (Snippets & Notes)"]
                context_section_header_str = "\n".join(context_section_lines) + "\n"
                context_section_header_tokens = len(enc.encode(context_section_header_str))

                temp_context_md_parts = []
                current_context_section_tokens = 0
                
                if current_total_tokens + context_section_header_tokens <= max_total_tokens:
                    for meta, dist in context_items_data:
                        item_md_lines = _format_context_item_for_mdc(meta)
                        item_md_segment = "\n".join(item_md_lines) + "\n" # Ensure newline after each item
                        item_tokens = len(enc.encode(item_md_segment))

                        if current_total_tokens + context_section_header_tokens + current_context_section_tokens + item_tokens <= max_total_tokens:
                            temp_context_md_parts.append(item_md_segment)
                            current_context_section_tokens += item_tokens
                        else:
                            logging.info(f"Context item (type: {meta.get('type')}) exceeds token budget. Stopping context additions.")
                            break
                    
                    if temp_context_md_parts:
                        full_context_section_str = context_section_header_str + "".join(temp_context_md_parts) + "\n"
                        actual_full_context_section_tokens = len(enc.encode(full_context_section_str))

                        if current_total_tokens + actual_full_context_section_tokens <= max_total_tokens:
                            final_mdc_parts.append(full_context_section_str)
                            current_total_tokens += actual_full_context_section_tokens
                        else:
                            logging.warning("Constructed context section exceeded budget on final check. Skipping context section.")
                else:
                    logging.info("Not enough token budget for Relevant Context section header. Skipping.")
            else:
                logging.info(f"No context items (snippets/notes) found matching query '{context_item_query}'.")
        except Exception as e:
            logging.error(f"Error searching or processing context items for mdc: {e}")

    # Final content assembly
    content = "".join(final_mdc_parts).strip() # Use strip() to remove any trailing newlines from the last added part
    if not content.endswith("\n") and content != "": # Ensure a single trailing newline if content exists
        content += "\n"
        
    # The current_total_tokens is the most accurate count of the final content if all additions were successful.
    # For verification, we can re-encode the final content.
    final_token_count_check = len(enc.encode(content))

    # Output to .cursor/rules/memory.mdc
    try:
        out_path = ROOT / ".cursor" / "rules" / "memory.mdc"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(content, encoding="utf-8")
        # Use current_total_tokens for reported count as it's accumulated, or final_token_count_check
        print(f"✅ memory.mdc updated ({out_path}). Tokens: ≈ {final_token_count_check} (Max: {max_total_tokens}) Accumulated: {current_total_tokens}")
    except IOError as e:
        logging.error(f"Failed to write memory.mdc file to {out_path}: {e}")
        print(f"❌ Error writing memory.mdc. Check logs.")
        sys.exit(1)
    except Exception as e:
        logging.critical(f"Unexpected error writing memory.mdc: {e}", exc_info=True)
        print(f"❌ Unexpected error writing memory.mdc. Check logs.")
        sys.exit(1)

if __name__ == "__main__":
    make()